{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "21fadf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import psutil\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a55135",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55f9d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================================================== #\n",
    "# --------------------------------------- CONDITIONAL PROBABILITY TABLES (CPT) --------------------------------------- #\n",
    "# ==================================================================================================================== #\n",
    "class CPT:\n",
    "    def __init__(self, head, parents):\n",
    "        self.head = head\n",
    "        self.parents = parents\n",
    "        self.entries = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        comma = \", \"\n",
    "        if len(self.parents) == 0:\n",
    "            return f\"probability ( {self.head.name} ) {{\" + \"\\n\" \\\n",
    "                                                            f\"  table {comma.join(map(str, self.entries[tuple()].values()))};\" + \"\\n\" \\\n",
    "                                                                                                                                 f\"}}\" + \"\\n\"\n",
    "        else:\n",
    "            return f\"probability ( {self.head.name} | {comma.join([p.name for p in self.parents])} ) {{\" + \"\\n\" + \\\n",
    "                \"\\n\".join([ \\\n",
    "                    f\"  ({comma.join(names)}) {comma.join(map(str, values.values()))};\" \\\n",
    "                    for names, values in self.entries.items() \\\n",
    "                    ]) + \"\\n}\\n\"\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# --------------------------------------------- VARIABLES REPRESENTATION --------------------------------------------- #\n",
    "# ==================================================================================================================== #\n",
    "class Variable:\n",
    "    def __init__(self, name, values):\n",
    "        self.name = name\n",
    "        self.values = values\n",
    "        self.cpt = None\n",
    "\n",
    "    def __str__(self):\n",
    "        comma = \", \"\n",
    "        return f\"variable {self.name} {{\" + \"\\n\" \\\n",
    "            + f\"  type discrete [ {len(self.values)} ] {{ {(comma.join(self.values))} }};\" + \"\\n\" \\\n",
    "            + f\"}}\" + \"\\n\"\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# ---------------------------------------------- BAYESIAN NETWORK CLASS ---------------------------------------------- #\n",
    "# ==================================================================================================================== #\n",
    "class BayesianNetwork:\n",
    "    def __init__(self, input_file):\n",
    "        with open(input_file) as f:\n",
    "            lines = f.readlines()\n",
    "        self.variables = {}\n",
    "        for i in range(len(lines)):\n",
    "            lines[i] = lines[i].lstrip().rstrip().replace('/', '-')\n",
    "        i = 0\n",
    "        while i < len(lines) and not lines[i].startswith(\"probability\"):\n",
    "            if lines[i].startswith(\"variable\"):\n",
    "                variable_name = lines[i].rstrip().split(' ')[1]\n",
    "                i += 1\n",
    "                variable_def = lines[i].rstrip().split(' ')\n",
    "                assert (variable_def[1] == 'discrete')\n",
    "                variable_values = [x for x in variable_def[6:-1]]\n",
    "                for j in range(len(variable_values)):\n",
    "                    variable_values[j] = re.sub(r'\\(|\\)|,', '', variable_values[j])\n",
    "                variable = Variable(variable_name, variable_values)\n",
    "                self.variables[variable_name] = variable\n",
    "            i += 1\n",
    "        while i < len(lines):\n",
    "            if lines[i].startswith('probability'):\n",
    "                split = lines[i].split(' ')\n",
    "                target_variable_name = split[2]\n",
    "                variable = self.variables[target_variable_name]\n",
    "                parents = [self.variables[x.rstrip().lstrip().replace(',', '')] for x in split[4:-2]]\n",
    "                assert (variable.name == split[2])\n",
    "                cpt = CPT(variable, parents)\n",
    "                i += 1\n",
    "                if len(parents) > 0:\n",
    "                    nb_lines = 1\n",
    "                    for p in parents:\n",
    "                        nb_lines *= len(p.values)\n",
    "                    for _ in range(nb_lines):\n",
    "                        cpt_line = [x for x in re.sub(r'\\(|\\)|,', '', lines[i][:-1]).split()]\n",
    "                        parent_values = tuple([x for x in cpt_line[:len(parents)]])\n",
    "                        probabilities = [float(p) for p in cpt_line[len(parents):]]\n",
    "                        cpt.entries[parent_values] = {v: p for v, p in zip(variable.values, probabilities)}\n",
    "                        i += 1\n",
    "                else:\n",
    "                    cpt_line = [x for x in re.sub(r'\\(|\\)|,', '', lines[i][:-1]).split()]\n",
    "                    probabilities = [float(p) for p in cpt_line[1:]]\n",
    "                    cpt.entries[tuple()] = {v: p for v, p in zip(variable.values, probabilities)}\n",
    "                variable.cpt = cpt\n",
    "            i += 1\n",
    "\n",
    "    def write(self, filename):\n",
    "        with open(filename, \"w\") as file:\n",
    "            for var in self.variables.values():\n",
    "                file.write(str(var))\n",
    "            for var in self.variables.values():\n",
    "                file.write(str(var.cpt))\n",
    "\n",
    "    def P_Yisy_given_parents_x(self, Y, y, x=tuple()):\n",
    "        return self.variables[Y].cpt.entries[x][y]\n",
    "\n",
    "    def P_Yisy_given_parents(self, Y, y, pa={}):\n",
    "        x = tuple([pa[parent.name] for parent in self.variables[Y].cpt.parents])\n",
    "        return self.P_Yisy_given_parents_x(Y, y, x)\n",
    "\n",
    "    def _get_children(self):\n",
    "        \"\"\"\n",
    "        Builds a mapping from each variable to its children in the Bayesian Network.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where each key is a variable name and the value is a list of its children.\n",
    "        \"\"\"\n",
    "        children = defaultdict(list)\n",
    "        for var in self.variables.values():\n",
    "            for parent in var.cpt.parents:\n",
    "                children[parent.name].append(var.name)\n",
    "        return children\n",
    "\n",
    "    def _normalize(self, dist):\n",
    "        \"\"\"\n",
    "        Normalizes a probability distribution so that the values sum to 1.\n",
    "\n",
    "        Args:\n",
    "            dist (dict): A dictionary mapping values to unnormalized probabilities.\n",
    "\n",
    "        Returns:\n",
    "            dict: The normalized distribution, or an empty dict if the total is 0.\n",
    "        \"\"\"\n",
    "        total = sum(dist.values())\n",
    "        return {k: v / total for k, v in dist.items() if total > 0}\n",
    "\n",
    "    def _get_pi_contribution(self, pname, pval, pi_msgs, evidence):\n",
    "        \"\"\"\n",
    "        Computes the π-contribution of a parent variable for belief propagation.\n",
    "\n",
    "        Args:\n",
    "            pname (str): The name of the parent variable.\n",
    "            pval (str): The value of the parent variable being considered.\n",
    "            pi_msgs (dict): Dictionary of π-messages already computed.\n",
    "            evidence (dict): Observed values for some variables.\n",
    "\n",
    "        Returns:\n",
    "            float: The π-contribution for the given parent value.\n",
    "        \"\"\"\n",
    "        if pname in pi_msgs:\n",
    "            return pi_msgs[pname][pval]\n",
    "        elif pname in evidence:\n",
    "            return 1.0 if evidence[pname] == pval else 0.0\n",
    "        else:\n",
    "            return 1.0 / len(self.variables[pname].values)\n",
    "\n",
    "    def _get_vals(self, pname, xi=None, focus_node=None, evidence=None):\n",
    "        \"\"\"\n",
    "        Returns possible values for a parent variable during message passing.\n",
    "\n",
    "        Args:\n",
    "            pname (str): The name of the parent variable.\n",
    "            xi (str, optional): The current value of the focus node (if applicable).\n",
    "            focus_node (str, optional): The node currently receiving messages.\n",
    "            evidence (dict): Observed evidence.\n",
    "\n",
    "        Returns:\n",
    "            list: List of possible values for pname, based on evidence and context.\n",
    "        \"\"\"\n",
    "        if evidence is None:\n",
    "            evidence = {}\n",
    "        if pname == focus_node:\n",
    "            return [xi]\n",
    "        elif pname in evidence:\n",
    "            return [evidence[pname]]\n",
    "        else:\n",
    "            return self.variables[pname].values\n",
    "\n",
    "    def _send_messages_to_root(self, node, evidence, children, lambda_msgs):\n",
    "        \"\"\"\n",
    "        Sends λ-messages from the leaves up to the given node in the Bayesian Network.\n",
    "\n",
    "        This function recursively computes λ-messages for a node by aggregating messages\n",
    "        from its children, incorporating evidence when available, as part of belief propagation.\n",
    "\n",
    "        Args:\n",
    "            node (str): The variable to which λ-messages are being sent.\n",
    "            evidence (dict): Observed values for some variables.\n",
    "            children (dict): A mapping from each variable to its list of children.\n",
    "            lambda_msgs (dict): Stores already computed λ-messages per variable.\n",
    "\n",
    "        Returns:\n",
    "            dict: The λ-message for the given node, mapping each of its values to a likelihood.\n",
    "        \"\"\"\n",
    "        # Avoid redundant computation by reusing cached λ-message\n",
    "        if node in lambda_msgs:\n",
    "            return lambda_msgs[node]\n",
    "        var = self.variables[node]\n",
    "        lambda_msg = {val: 1.0 for val in var.values}\n",
    "        for child in children[node]:\n",
    "            child_lambda = self._send_messages_to_root(child, evidence, children, lambda_msgs)\n",
    "            child_var = self.variables[child]\n",
    "            parent_names = [p.name for p in child_var.cpt.parents]\n",
    "            new_lambda = {}\n",
    "            for xi in var.values:\n",
    "                msg = 0.0\n",
    "                for xj in child_var.values:\n",
    "                    # Collect possible values for each parent, constrained by evidence and current xi\n",
    "                    all_pa_vals = itertools.product(\n",
    "                        *[self._get_vals(pname, xi, node, evidence) for pname in parent_names])\n",
    "                    for pa in all_pa_vals:\n",
    "                        pa_dict = dict(zip(parent_names, pa))\n",
    "                        pa_vals = tuple(pa_dict[p.name] for p in child_var.cpt.parents)\n",
    "                        prob = child_var.cpt.entries[pa_vals][xj]\n",
    "                        msg += prob * child_lambda[xj]\n",
    "                new_lambda[xi] = lambda_msg[xi] * msg\n",
    "            lambda_msg = new_lambda\n",
    "        # Override λ-message with hard evidence if the variable is observed\n",
    "        if node in evidence:\n",
    "            observed = evidence[node]\n",
    "            lambda_msg = {val: (1.0 if val == observed else 0.0) for val in var.values}\n",
    "        lambda_msgs[node] = lambda_msg\n",
    "        return lambda_msg\n",
    "\n",
    "    def _send_messages_from_root(self, node, pi_msg, evidence, children, lambda_msgs, beliefs, pi_msgs):\n",
    "        \"\"\"\n",
    "        Sends π-messages from the root to all its descendants in the Bayesian Network.\n",
    "\n",
    "        This function computes beliefs for all reachable nodes starting from the root,\n",
    "        using upward (λ) and downward (π) message passing as defined by Pearl's belief propagation algorithm.\n",
    "\n",
    "        Args:\n",
    "            node (str): The current variable from which messages are sent.\n",
    "            pi_msg (dict): The π-message from the parent to the current node (prior belief).\n",
    "            evidence (dict): Observed values for some variables.\n",
    "            children (dict): A mapping from each variable to its children.\n",
    "            lambda_msgs (dict): Precomputed λ-messages from children to parents.\n",
    "            beliefs (dict): Stores the resulting marginal beliefs per variable.\n",
    "            pi_msgs (dict): Stores the π-messages sent to each variable.\n",
    "\n",
    "        Returns:\n",
    "            None: Updates `beliefs` and `pi_msgs` in place.\n",
    "        \"\"\"\n",
    "        var = self.variables[node]\n",
    "        lambda_msg = lambda_msgs[node]\n",
    "        # Combine π and λ messages to compute the final belief for this node\n",
    "        belief = {val: pi_msg[val] * lambda_msg[val] for val in var.values}\n",
    "        beliefs[node] = self._normalize(belief)\n",
    "        pi_msgs[node] = pi_msg\n",
    "        for child in children[node]:\n",
    "            child_var = self.variables[child]\n",
    "            parent_names = [p.name for p in child_var.cpt.parents]\n",
    "            # Initialize π-message for each possible value of the child\n",
    "            child_pi = {xj: 0.0 for xj in child_var.values}\n",
    "            for xj in child_var.values:\n",
    "                total = 0.0\n",
    "                # Determine all parent value combinations, constrained by evidence\n",
    "                all_pa_vals = itertools.product(\n",
    "                    *[self._get_vals(pname, None, None, evidence) for pname in parent_names])\n",
    "                for pa in all_pa_vals:\n",
    "                    pa_dict = dict(zip(parent_names, pa))\n",
    "                    pa_vals = tuple(pa_dict[p.name] for p in child_var.cpt.parents)\n",
    "                    prob = child_var.cpt.entries[pa_vals][xj]\n",
    "                    # Compute contribution of each parent value using upstream π-messages\n",
    "                    contrib = 1.0\n",
    "                    for pname, pval in pa_dict.items():\n",
    "                        contrib *= self._get_pi_contribution(pname, pval, pi_msgs, evidence)\n",
    "                    total += contrib * prob\n",
    "                child_pi[xj] = total\n",
    "            # Recurse on the child node with the computed π-message\n",
    "            self._send_messages_from_root(child, child_pi, evidence, children, lambda_msgs, beliefs, pi_msgs)\n",
    "\n",
    "    def query_marginal(self, query_var, evidence):\n",
    "        \"\"\"\n",
    "        Computes the marginal distribution of a variable given observed evidence using belief propagation.\n",
    "\n",
    "        Args:\n",
    "            query_var (str): The name of the variable to query.\n",
    "            evidence (dict): A dictionary mapping observed variable names to their values.\n",
    "\n",
    "        Returns:\n",
    "            dict: A probability distribution over the values of `query_var`, normalized to sum to 1.\n",
    "        \"\"\"\n",
    "        children = self._get_children()\n",
    "        lambda_msgs = {}\n",
    "        beliefs = {}\n",
    "        pi_msgs = {}\n",
    "        root = next(iter(evidence)) if evidence else query_var\n",
    "        self._send_messages_to_root(root, evidence, children, lambda_msgs)\n",
    "        root_var = self.variables[root]\n",
    "        # If root has parents, no prior is available, so assume uniform prior\n",
    "        if root_var.cpt.parents:\n",
    "            root_pi = {val: 1.0 for val in root_var.values}\n",
    "        else:\n",
    "            root_pi = root_var.cpt.entries[tuple()]\n",
    "        self._send_messages_from_root(root, root_pi, evidence, children, lambda_msgs, beliefs, pi_msgs)\n",
    "        # Fallback in case the target query_var wasn't covered in the first propagation\n",
    "        if query_var not in beliefs:\n",
    "            self._send_messages_to_root(query_var, evidence, children, lambda_msgs)\n",
    "            # Again, use uniform prior if the query_var has parents\n",
    "            if self.variables[query_var].cpt.parents:\n",
    "                root_pi = {val: 1.0 for val in self.variables[query_var].values}\n",
    "            else:\n",
    "                root_pi = self.variables[query_var].cpt.entries[tuple()]\n",
    "            self._send_messages_from_root(query_var, root_pi, evidence, children, lambda_msgs, beliefs, pi_msgs)\n",
    "        return beliefs[query_var]\n",
    "\n",
    "    def query_joint(self, var1, var2, evidence):\n",
    "        \"\"\"\n",
    "        Computes the joint probability distribution of two variables given partial evidence.\n",
    "\n",
    "        Uses belief propagation to compute:\n",
    "            P(var1, var2 | evidence) = P(var1 | var2, evidence) * P(var2 | evidence)\n",
    "\n",
    "        Args:\n",
    "            var1 (str): The name of the first variable.\n",
    "            var2 (str): The name of the second variable.\n",
    "            evidence (dict): Observed values for other variables in the network.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary {val1: {val2: prob}} representing the normalized joint distribution.\n",
    "        \"\"\"\n",
    "        joint_dist = {}\n",
    "        for val1 in self.variables[var1].values:\n",
    "            joint_dist[val1] = {}\n",
    "            for val2 in self.variables[var2].values:\n",
    "                extended_evidence = dict(evidence)\n",
    "                extended_evidence[var1] = val1\n",
    "                extended_evidence[var2] = val2\n",
    "                # Fallback in case var2 is not in the marginal or val2 is missing\n",
    "                marg = self.query_marginal(var2, evidence)\n",
    "                if var2 not in marg or val2 not in marg:\n",
    "                    marg = self.query_marginal(var2, evidence)\n",
    "                if val2 not in marg:\n",
    "                    continue\n",
    "                # Fallback in case var1 is not in the conditional or val1 is missing\n",
    "                cond = self.query_marginal(var1, {**evidence, var2: val2})\n",
    "                if var1 not in cond or val1 not in cond:\n",
    "                    cond = self.query_marginal(var1, {**evidence, var2: val2})\n",
    "                joint_dist[val1][val2] = cond[val1] * marg[val2]\n",
    "        return self._normalize_nested(joint_dist)\n",
    "\n",
    "    def _normalize_nested(self, joint_dist):\n",
    "        \"\"\"\n",
    "        Normalizes a nested joint distribution so that all probabilities sum to 1.\n",
    "\n",
    "        Args:\n",
    "            joint_dist (dict): Nested dictionary {val1: {val2: prob}}.\n",
    "\n",
    "        Returns:\n",
    "            dict: Normalized joint distribution. If total is 0, returns the original.\n",
    "        \"\"\"\n",
    "        # Flatten all probabilities to compute total\n",
    "        total = sum(v for d in joint_dist.values() for v in d.values())\n",
    "        if total == 0:\n",
    "            return joint_dist\n",
    "        # Normalize each inner value manually by dividing by the total\n",
    "        for k1 in joint_dist:\n",
    "            joint_dist[k1] = self._normalize({k2: v / total for k2, v in joint_dist[k1].items()})\n",
    "        return joint_dist\n",
    "\n",
    "    def learn_parameters(self, df, laplace_smoothing=True):\n",
    "        \"\"\"\n",
    "        Learns the parameters (CPTs) of the Bayesian network from a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataset with complete variable assignments (no missing values).\n",
    "            laplace_smoothing (bool): Whether to apply Laplace smoothing (default: True).\n",
    "\n",
    "        Returns:\n",
    "            None: Updates CPTs of each variable in-place.\n",
    "        \"\"\"\n",
    "        alpha = 1 if laplace_smoothing else 0\n",
    "        for var in self.variables.values():\n",
    "            # in case of missing CPT\n",
    "            if var.cpt is None:\n",
    "                var.cpt = CPT(var, [])\n",
    "        for var_name, var in self.variables.items():\n",
    "            if var.cpt is None:\n",
    "                continue\n",
    "            parents = var.cpt.parents\n",
    "            cpt = {}\n",
    "            parent_names = [p.name for p in parents]\n",
    "            parent_value_combinations = list(itertools.product(*[self.variables[p].values for p in parent_names]))\n",
    "            for parent_vals in parent_value_combinations:\n",
    "                parent_vals = tuple(parent_vals)\n",
    "                counts = {v: 0 for v in var.values}\n",
    "                total = 0\n",
    "                # Select rows where parent values match\n",
    "                condition = (df[parent_names] == list(parent_vals)).all(axis=1)\n",
    "                matching_rows = df[condition]\n",
    "                for y_val in matching_rows[var_name]:\n",
    "                    if y_val in counts:\n",
    "                        counts[y_val] += 1\n",
    "                        total += 1\n",
    "                smoothed_total = total + alpha * len(var.values)\n",
    "                probs = {v: (counts[v] + alpha) / smoothed_total for v in var.values}\n",
    "                cpt[parent_vals] = probs\n",
    "            if not parents:\n",
    "                counts = {v: 0 for v in var.values}\n",
    "                total = 0\n",
    "                for y_val in df[var_name]:\n",
    "                    if y_val in counts:\n",
    "                        counts[y_val] += 1\n",
    "                        total += 1\n",
    "                smoothed_total = total + alpha * len(var.values)\n",
    "                probs = {v: (counts[v] + alpha) / smoothed_total for v in var.values}\n",
    "                cpt[tuple()] = probs\n",
    "            var.cpt.entries = cpt\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "# ------------------------------------------------- PROJECT PIPELINE ------------------------------------------------- #\n",
    "# ==================================================================================================================== #\n",
    "def _clean_missing_dataframe(df):\n",
    "    \"\"\"\n",
    "    Cleans a dataframe containing missing values and numeric strings.\n",
    "\n",
    "    Replaces \"?\" with NaN, and converts all non-missing values to stringified integers.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe with potential missing values and mixed types.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned dataframe with consistent string integer values and NaNs.\n",
    "    \"\"\"\n",
    "    df = df.replace(\"?\", pd.NA)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(lambda x: str(int(float(x))) if pd.notna(x) else pd.NA)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _evaluate_network(bn, test_df, miss_df, testbar):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy and average confidence of a Bayesian Network on test data with missing values.\n",
    "\n",
    "    Args:\n",
    "        bn (BayesianNetwork): The trained Bayesian network.\n",
    "        test_df (pd.DataFrame): Dataset with complete ground truth values.\n",
    "        miss_df (pd.DataFrame): Same dataset with some values replaced by NaN.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy, the proportion of correctly predicted missing values.\n",
    "        float: Average confidence of the predicted values.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    confidences = []\n",
    "    testbar.reset(total=len(miss_df))\n",
    "    testbar.iterable = range(len(miss_df))\n",
    "    testbar.set_description(\"Evaluating\")\n",
    "    start_cpu = time.process_time()\n",
    "    inferences = 0\n",
    "    for i in testbar:\n",
    "        evidence = miss_df.iloc[i].dropna().to_dict()\n",
    "        ground_truth = test_df.iloc[i].to_dict()\n",
    "        missing_vars = [col for col in miss_df.columns if pd.isna(miss_df.iloc[i][col])]\n",
    "        inferences += len(missing_vars)\n",
    "        for var in missing_vars:\n",
    "            predicted_dist = bn.query_marginal(var, evidence)\n",
    "            if not predicted_dist:\n",
    "                continue\n",
    "            predicted_val = max(predicted_dist, key=predicted_dist.get)\n",
    "            confidence = predicted_dist[predicted_val]\n",
    "            confidences.append(confidence)\n",
    "            is_correct = (predicted_val == str(ground_truth[var]))\n",
    "            correct += is_correct\n",
    "            total += 1\n",
    "        testbar.update(1)\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "    return accuracy, avg_confidence, (time.process_time() - start_cpu) / inferences, inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7a0bf",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1698fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../\"\n",
    "DATA_PATH  = ROOT + \"datasets/\"\n",
    "MODEL_PATH = ROOT + \"models/\"\n",
    "RESULTS_PATH = ROOT + \"results/\"\n",
    "DATASETS = [\"alarm\", \"andes\", \"asia\", \"sachs\", \"sprinkler\", \"water\"]\n",
    "MODELS = [\"bnlearn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66bcdfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_complexity(bn):\n",
    "    \"\"\"\n",
    "    Computes model complexity metrics for a Bayesian Network:\n",
    "    number of edges, average degree, depth, and max width.\n",
    "\n",
    "    Args:\n",
    "        bn (BayesianNetwork): The network to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (num_edges, avg_degree, depth, width)\n",
    "    \"\"\"\n",
    "    from collections import defaultdict, deque\n",
    "\n",
    "    # ── Graph representation ─────────────────────────────\n",
    "    graph = defaultdict(list)\n",
    "    for child_name, child_var in bn.variables.items():\n",
    "        if child_var.cpt:\n",
    "            for parent in child_var.cpt.parents:\n",
    "                graph[parent.name].append(child_name)\n",
    "\n",
    "    all_nodes = list(bn.variables.keys())\n",
    "    num_nodes = len(all_nodes)\n",
    "    num_edges = sum(len(children) for children in graph.values())\n",
    "    avg_degree = num_edges / num_nodes if num_nodes > 0 else 0\n",
    "\n",
    "    # ── Topological traversal to compute depth and width ─\n",
    "    in_degrees = {v: 0 for v in all_nodes}\n",
    "    for parent in graph:\n",
    "        for child in graph[parent]:\n",
    "            in_degrees[child] += 1\n",
    "\n",
    "    roots = [v for v, deg in in_degrees.items() if deg == 0]\n",
    "    depth_dict = {v: 0 for v in roots}\n",
    "    width_at_depth = defaultdict(int)\n",
    "\n",
    "    queue = deque(roots)\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        d = depth_dict[node]\n",
    "        width_at_depth[d] += 1\n",
    "        for child in graph[node]:\n",
    "            depth_dict[child] = d + 1\n",
    "            queue.append(child)\n",
    "\n",
    "    max_depth = max(depth_dict.values()) if depth_dict else 0\n",
    "    max_width = max(width_at_depth.values()) if width_at_depth else 0\n",
    "\n",
    "    return num_edges, avg_degree, max_depth, max_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "06322e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_structural_quality(bn, gt):\n",
    "    \"\"\"\n",
    "    Computes structural quality metrics between a learned Bayesian network and a ground truth:\n",
    "    Structural Hamming Distance (SHD), edge precision, and edge recall.\n",
    "\n",
    "    Args:\n",
    "        bn (BayesianNetwork): Learned Bayesian network.\n",
    "        gt (BayesianNetwork): Ground truth Bayesian network.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (shd, precision, recall)\n",
    "    \"\"\"\n",
    "    def get_edges(net):\n",
    "        edges = set()\n",
    "        for child, var in net.variables.items():\n",
    "            if var.cpt:\n",
    "                for parent in var.cpt.parents:\n",
    "                    edges.add((parent.name, child))\n",
    "        return edges\n",
    "\n",
    "    edges_bn = get_edges(bn)\n",
    "    edges_gt = get_edges(gt)\n",
    "\n",
    "    tp = len(edges_bn & edges_gt)\n",
    "    fp = len(edges_bn - edges_gt)\n",
    "    fn = len(edges_gt - edges_bn)\n",
    "    \n",
    "    shd = fp + fn\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return shd, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "646d6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(bn, df):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of a dataset given a Bayesian network.\n",
    "\n",
    "    Args:\n",
    "        bn (BayesianNetwork): The Bayesian network with learned parameters.\n",
    "        df (pd.DataFrame): The complete dataset to evaluate (e.g., test_df).\n",
    "\n",
    "    Returns:\n",
    "        float: The average log-likelihood over all rows in the dataset.\n",
    "    \"\"\"\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        row_prob = 1.0\n",
    "\n",
    "        for var_name, var in bn.variables.items():\n",
    "            if var.cpt is None:\n",
    "                continue\n",
    "\n",
    "            parents = var.cpt.parents\n",
    "            parent_vals = tuple(row_dict[p.name] for p in parents)\n",
    "            value = row_dict[var_name]\n",
    "\n",
    "            # Ensure value is a string if the CPT uses str keys\n",
    "            prob = var.cpt.entries.get(parent_vals, {}).get(str(value), 1e-9)\n",
    "            row_prob *= prob\n",
    "\n",
    "        log_likelihood += np.log(row_prob + 1e-9)  # Add epsilon to avoid log(0)\n",
    "        n += 1\n",
    "\n",
    "    return log_likelihood / n if n > 0 else float(\"-inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_parameters(bn):\n",
    "    \"\"\"\n",
    "    Counts the total number of free parameters in the Bayesian network.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of conditional probabilities to estimate.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for var in bn.variables.values():\n",
    "        if var.cpt is None:\n",
    "            continue\n",
    "        num_parent_combinations = 1\n",
    "        for parent in var.cpt.parents:\n",
    "            num_parent_combinations *= len(parent.values)\n",
    "        total += num_parent_combinations * (len(var.values) - 1)  # Last value is determined\n",
    "    return total\n",
    "\n",
    "\n",
    "def compute_bic_aic(log_likelihood, num_parameters, num_samples):\n",
    "    \"\"\"\n",
    "    Computes BIC and AIC scores given log-likelihood, number of parameters, and data size.\n",
    "\n",
    "    Args:\n",
    "        log_likelihood (float): Average log-likelihood over the dataset.\n",
    "        num_parameters (int): Number of free parameters in the Bayesian network.\n",
    "        num_samples (int): Number of rows in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (bic, aic)\n",
    "    \"\"\"\n",
    "    total_log_likelihood = log_likelihood * num_samples\n",
    "    bic = -2 * total_log_likelihood + num_parameters * np.log(num_samples)\n",
    "    aic = -2 * total_log_likelihood + 2 * num_parameters\n",
    "    return bic, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad288f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fabc6eaae149b9842b603869ac5362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8afac47f0f485094d94bcba26d17ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models over `alarm`:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdce14637c8494f92fb30721a61393f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "`bnlearn` stability:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dcc75675164187b6981a2aebd88e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5430066ab0b4f9ab818e28854cf4faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb21da2f8e54401914984feec3ea160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3c8cc424894a4b8003f0fcbaa3902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077e4e07266b453a95664cdc89217175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b0286e6136480ca296b2d6320f5b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fb1e80da6f4f9a9b03b4a29f761b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69a643ebeb24d44813cdf2a63b88b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b10f3e1c545caabf97f8ed6e4f068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61259551ac742288fabbbb8713823a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bd29a7932c4a4b97a216c307177475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5a09a6633b40fea37b582e7411fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629a58ead74a47709bd9c8d051971b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a793016b70b4de98fb3b39f1f55e982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723e4413f3124e9da7b1da3ea511c920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331e6d73629442a995b187f527e78fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7ad2865a924e3f8c03616166ee313b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b383b29615804e35b3b2fe938046ab26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa3b4a714a1436c96d87ad1ed5b4c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dadca44638643b8aa1d2de2dad7b756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9b422b7a5b4d45aa79ada97ddc358f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models over `sprinkler`:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a779230cd9f4793841eeabe77e68ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "`bnlearn` stability:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d873386e69a4372a231fad6e619e88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd0f34744ae4893a39bae998d49a94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e578c66d2ff4a3eaa9489781a6e9171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50899c8092ee4c2bbfa0813143eca962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c86b9302934b99ae2ad06903845ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4024b2001b04a7bb001304d129086cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151cb8b3a934439b973384945323c7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6313f421ab864516add8ef82c60b09f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8288248295564c058e0a8d15ac686608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c5804970bc43d88761c4874458781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e872b0b37f6d4dc090400f04ccad9aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342585dd8ef841658e82f536d1c37a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a262207061f4b7aa68b6b3fe2dc13bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f703a8308acd4ba094d522ce357a1edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed97e87a90547118b35024d90cfa83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92235c002aca4684ae0c63a10d1a7fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4003d1842a4813b3dfb47238f432ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69578fbe346b425eb313fb96dd68f608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model complexity:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8298c1c4b9d94b14ae34b525b6c80196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Structural quality:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947732ba93bd442b9cf1fb5033489601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `bnlearn`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "process = psutil.Process(os.getpid())\n",
    "\n",
    "for dataset in tqdm(DATASETS, desc=f\"Datasets\", position=0):\n",
    "    # ── Load Data ─────────────────────────────\n",
    "    train_df = pd.read_csv(DATA_PATH + dataset + \"/train.csv\")\n",
    "    test_df = pd.read_csv(DATA_PATH + dataset + \"/test.csv\")\n",
    "    miss_df = _clean_missing_dataframe(pd.read_csv(DATA_PATH + dataset + \"/test_missing.csv\"))\n",
    "\n",
    "    with tqdm(MODELS, desc=f\"Models over `{dataset}`\", leave=False, position=1) as evalbar:\n",
    "        for model in evalbar:\n",
    "            with tqdm(range(1, 6), desc=f\"`{model}` stability\", leave=False, position=2) as stabilitybar:\n",
    "                for i in stabilitybar:\n",
    "                    # ── Create Network ────────────────────────\n",
    "                    bn = BayesianNetwork(MODEL_PATH + dataset + \"/\" + dataset + \"_complete.bif\")\n",
    "                    # ── Train ─────────────────────────────────\n",
    "                    start_cpu = time.process_time()\n",
    "                    with tqdm(range(1), desc=f\"Training\", leave=False, position=3) as trainbar:\n",
    "                        for fold in trainbar:\n",
    "                            bn.learn_parameters(train_df)\n",
    "                            trainbar.update(1)\n",
    "                    training_time = time.process_time() - start_cpu\n",
    "                    # ── Save Network ────────────────────────\n",
    "                    bn.write(MODEL_PATH + dataset + \"/\" + model + \"_fold_\" + str(i) + \".bif\")\n",
    "                    # ── Model Complexity ──────────────────────────────────\n",
    "                    with tqdm(range(1), desc=f\"Model complexity\", leave=False, position=4) as complexitybar:\n",
    "                        number_edges, degree_avg, depth, width = compute_model_complexity(bn)\n",
    "                        complexitybar.update(1)\n",
    "                    # ── Structural quality ──────────────────────────────────\n",
    "                    with tqdm(range(2), desc=f\"Structural quality\", leave=False, position=5) as structuralbar:\n",
    "                        for _ in structuralbar:\n",
    "                            gt = BayesianNetwork(MODEL_PATH + dataset + \"/\" + dataset + \"_complete.bif\")\n",
    "                            structuralbar.update(1)\n",
    "                            shd, edge_precision, edge_recall = compute_structural_quality(bn, gt)\n",
    "                            structuralbar.update(1)\n",
    "                    # ── Log-likelihood ──────────────────────────────────\n",
    "                    log_likelihood = compute_log_likelihood(bn, test_df)\n",
    "                    # ── BIC and AIC ──────────────────────────────────\n",
    "                    num_parameters = count_model_parameters(bn)\n",
    "                    bic, aic = compute_bic_aic(log_likelihood, num_parameters, len(test_df))\n",
    "                    # ── Test ──────────────────────────────────\n",
    "                    inference_memory = []\n",
    "                    with tqdm(desc=f\"Testing `{model}`\", leave=False, position=6) as testbar:\n",
    "                        before_mem = process.memory_info().rss\n",
    "                        acc, conf, inf_time, number_inferences = _evaluate_network(bn, test_df, miss_df, testbar)\n",
    "                        after_mem = process.memory_info().rss\n",
    "                        memory_used = (after_mem - before_mem) / 1024 / 1024\n",
    "                        results.append({\n",
    "                            \"dataset\": dataset,\n",
    "                            \"model\": model,\n",
    "                            \"initialization\": i,\n",
    "                            \"train_time_mean\": training_time,\n",
    "                            \"edges\": number_edges,\n",
    "                            \"avg_degree\": degree_avg,\n",
    "                            \"depth_mean\": depth,\n",
    "                            \"width_mean\": width,\n",
    "                            \"shd\": shd,\n",
    "                            \"edge_precision\": edge_precision,\n",
    "                            \"edge_recall\": edge_recall,\n",
    "                            \"inf_memory_mean\": memory_used,\n",
    "                            \"inf_time_mean\": inf_time,\n",
    "                            \"inferences\": number_inferences,\n",
    "                            \"acc\": acc,\n",
    "                            \"conf\": conf,\n",
    "                            \"log_likelihood\": log_likelihood,\n",
    "                            \"bic\": bic,\n",
    "                            \"aic\": aic,\n",
    "                            \"num_parameters\": num_parameters,\n",
    "                        })\n",
    "                        testbar.close()\n",
    "                    structuralbar.close()\n",
    "                    complexitybar.close()\n",
    "                    trainbar.close()\n",
    "        evalbar.close()\n",
    "\n",
    "results_df = pd.DataFrame(results\n",
    "results_df.to_csv(RESULTS_PATH + \"models_comparison_over_datasets.csv\", index=False)\n",
    "print(f\"Results saved to {RESULTS_PATH}models_comparison_over_datasets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25b2ce",
   "metadata": {},
   "source": [
    "## Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77bce367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17a170d3dfa4f999b971d91c4218b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets          :   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fdf2ab26254c9dac4e9ab863e3b22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models over `alarm`:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287f89c9fb847f4897cc9239a33168c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model `complete`  :   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b2dbdf6d124458907d9dc6a7819d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing `complete`: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Sprinkler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     miss_df\u001b[38;5;241m.\u001b[39miat[i, j] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mNA\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m trainbar:\n\u001b[0;32m---> 33\u001b[0m     acc, conf, _, time \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmiss_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(dataset)\n\u001b[1;32m     36\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "Cell \u001b[0;32mIn[97], line 452\u001b[0m, in \u001b[0;36m_evaluate_network\u001b[0;34m(bn, test_df, miss_df, testbar)\u001b[0m\n\u001b[1;32m    450\u001b[0m inferences \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(missing_vars)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m missing_vars:\n\u001b[0;32m--> 452\u001b[0m     predicted_dist \u001b[38;5;241m=\u001b[39m \u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_marginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicted_dist:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[97], line 281\u001b[0m, in \u001b[0;36mBayesianNetwork.query_marginal\u001b[0;34m(self, query_var, evidence)\u001b[0m\n\u001b[1;32m    279\u001b[0m pi_msgs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    280\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(evidence)) \u001b[38;5;28;01mif\u001b[39;00m evidence \u001b[38;5;28;01melse\u001b[39;00m query_var\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_messages_to_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_msgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m root_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[root]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# If root has parents, no prior is available, so assume uniform prior\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[97], line 190\u001b[0m, in \u001b[0;36mBayesianNetwork._send_messages_to_root\u001b[0;34m(self, node, evidence, children, lambda_msgs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m lambda_msgs:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lambda_msgs[node]\n\u001b[0;32m--> 190\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    191\u001b[0m lambda_msg \u001b[38;5;241m=\u001b[39m {val: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m var\u001b[38;5;241m.\u001b[39mvalues}\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children[node]:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sprinkler'"
     ]
    }
   ],
   "source": [
    "MODELS = [\"complete\"]\n",
    "MISSES = {\n",
    "    \"alarm\": 0.5,\n",
    "    \"andes\": 0.5,\n",
    "    \"asia\": 0.5,\n",
    "    \"sachs\": 0.5,\n",
    "    \"sprinkler\": 0.5,\n",
    "    \"water\": 0.5,\n",
    "}\n",
    "CROSS_VALIDATION = 5\n",
    "STEP = 0.01\n",
    "\n",
    "for dataset in tqdm(DATASETS, desc=\"Datasets          \", position=0):\n",
    "    df = pd.read_csv(DATA_PATH + dataset + \"/train.csv\")\n",
    "    results = defaultdict(list)\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    max_missing = int(total_cells * MISSES[dataset])\n",
    "\n",
    "    for model in tqdm(MODELS, desc=f\"Models over `{dataset}`\", leave=False, position=1):\n",
    "\n",
    "        bn = BayesianNetwork(MODEL_PATH + dataset + \"/\" + model + \".bif\")\n",
    "\n",
    "        for n_missing in tqdm(range(1, max_missing + 1, max(1, int(total_cells * STEP))),\n",
    "                              desc=f\"Model `{model}`  \", leave=False, position=2):\n",
    "            miss_df = test_df.copy()\n",
    "\n",
    "            idx = [(i, j) for i in range(miss_df.shape[0]) for j in range(miss_df.shape[1])]\n",
    "            selected = random.sample(idx, n_missing)\n",
    "            for i, j in selected:\n",
    "                miss_df.iat[i, j] = pd.NA\n",
    "\n",
    "            with tqdm(desc=f\"Testing `{model}`\", leave=False, position=3) as trainbar:\n",
    "                acc, conf, _, time = _evaluate_network(bn, test_df, miss_df, trainbar)\n",
    "\n",
    "            results[\"dataset\"].append(dataset)\n",
    "            results[\"model\"].append(model)\n",
    "            results[\"missing\"].append(n_missing)\n",
    "            results[\"accuracy\"].append(acc)\n",
    "            results[\"confidence\"].append(conf)\n",
    "            results[\"time\"].append(time)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(RESULTS_PATH + \"robustness.csv\", index=False)\n",
    "print(f\"Saved robustness results to: {RESULTS_PATH}robustness.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
